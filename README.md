# ğŸ¤– Intelligent Code Generation Crew

**A Three-Agent System for Automated Python Development**




## ğŸš€ Quick Start Guide

### Prerequisites
- âœ… Ollama installed
- âœ… Python 3.8+
- âœ… 5 minutes of your time

---

## ğŸ“¦ Installation

### 1. Create Virtual Environment

```bash
# Navigate to project folder
cd code-test-crew

# Create virtual environment
python -m venv venv

# Activate it
# Windows:
venv\Scripts\activate

# Mac/Linux:
source venv/bin/activate
```

### 2. Install Dependencies

```bash
pip install requests
```

That's it! âœ…

---

## ğŸ® How to Run

### Step 1: Start Ollama Server

Open a **FIRST TERMINAL** and run:

```bash
ollama serve
```

âš ï¸ **IMPORTANT:** Keep this terminal open while using the system!

You should see:
```
Listening on 127.0.0.1:11434
```

---

### Step 2: Download a Model (First Time Only)

Open a **SECOND TERMINAL** and run:

```bash
# Check available models
ollama list

# If you don't have a model, download one:
# Option 1 (RECOMMENDED - Fast):
ollama pull gemma3:1b

# Option 2 (Good balance):
ollama pull llama3.2

# Option 3 (Most powerful):
ollama pull llama3:latest
```

---

### Step 3: Run the System

In the **SECOND TERMINAL** (with venv activated):

```bash
python main.py
```

---

## ğŸ¯ Using the System

### When the program starts:

1. **Choose a model:**
   ```
   Quel modÃ¨le Ollama utiliser ? (dÃ©faut: llama3.2) :
   ```
   - Press **Enter** to use default (llama3.2)
   - OR type `gemma3:1b` or `llama3:latest`

2. **Choose a problem:**
   ```
   Choisis un exemple (1-7) ou tape ton propre problÃ¨me :
   ```
   - Type `1` for factorial
   - Type `2` for palindrome
   - OR type your own problem in French or English

3. **Watch the agents work:**
   - ğŸ¤– Coder writes code
   - ğŸ§ª Tester tests the code
   - âœ… Success or ğŸ”„ Retry with feedback

4. **Save the code:**
   ```
   ğŸ’¾ Sauvegarder le code ? (o/n) :
   ```
   - Type `o` to save
   - Give it a filename (e.g., `factorial.py`)

---

## ğŸ“ Example Run

```bash
# Terminal 1
ollama serve

# Terminal 2
cd code-test-crew
source venv/bin/activate  # or venv\Scripts\activate on Windows
python main.py

# When prompted:
Quel modÃ¨le ? â†’ gemma3:1b
Choisis un exemple â†’ 1
# Wait ~30 seconds
# âœ… Code generated!
Sauvegarder ? â†’ o
Nom du fichier â†’ factorial.py
```

---

## ğŸ—‚ï¸ Project Structure

```
code-test-crew/
â”œâ”€â”€ agent_base.py          # Base class for all agents
â”œâ”€â”€ coder_agent.py         # Coder Agent implementation
â”œâ”€â”€ tester_agent.py        # Tester Agent V1 (static analysis)
â”œâ”€â”€ tester_agent_v2.py     # Tester Agent V2 (real execution)
â”œâ”€â”€ reviewer_agent.py      # Reviewer Agent (optional, Phase 2)
â”œâ”€â”€ orchestrator.py        # System orchestrator
â”œâ”€â”€ llm_client.py          # Ollama client
â”œâ”€â”€ main.py               # Main entry point
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md             # This file
```

---

## ğŸ”§ Troubleshooting

### Problem: "Impossible de se connecter Ã  Ollama"

**Solution:**
```bash
# Make sure Ollama is running in Terminal 1
ollama serve
```

### Problem: "ModuleNotFoundError: No module named 'requests'"

**Solution:**
```bash
# Make sure venv is activated (you should see (venv) in your prompt)
pip install requests
```

### Problem: "Model not found"

**Solution:**
```bash
# Download the model you're trying to use
ollama pull gemma3:1b
```

### Problem: "Timeout - Le modÃ¨le met trop de temps"

**Solution:**
- Use a smaller model: `gemma3:1b` (fastest)
- OR increase timeout in `llm_client.py` line 20: change `timeout=60` to `timeout=300`

---

## ğŸ® Available Models

| Model           | Size | Speed | Quality |
|-------          |------|-------|---------|
| `gemma3:1b`     | 815 MB | âš¡âš¡âš¡ Very Fast | â­â­ Good |
| `llama3.2`      | 2 GB    | âš¡âš¡ Fast | â­â­â­ Very Good |
| `llama3:latest` | 4.7 GB | âš¡ Slower | â­â­â­â­ Excellent |

**Recommendation:** Start with `gemma3:1b` for testing, use `llama3.2` for best results.

---

## ğŸ“Š Evaluation Metrics

The system tracks:
- **Pass Rate:** Percentage of functions that work (Target: 70%+)
- **First-Try Success:** Functions that work immediately
- **Average Attempts:** Mean iterations needed (Target: < 2.5)
- **Failure Analysis:** Why failures occur (syntax, logic, edge cases)

## ğŸ†˜ Need Help?

1. Check the Troubleshooting section above
2. Make sure Ollama is running: `ollama serve`
3. Make sure venv is activated: you should see `(venv)` in your terminal
4. Check that your model is installed: `ollama list`

---

## ğŸ‰ Quick Test

Want to quickly test if everything works?

```bash
# Terminal 1
ollama serve

# Terminal 2
cd code-test-crew
source venv/bin/activate
python main.py
# Press Enter twice (use defaults)
# Type: 1
# Wait ~30 seconds
# Should see: âœ… SUCCÃˆS !
```

If you see success, you're all set! ğŸš€

---

**Last Updated:** November 17, 2025